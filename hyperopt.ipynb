{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin, space_eval, Trials\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV Metrics: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "HyperOpt: https://github.com/hyperopt/hyperopt/wiki/FMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5 # for cross validation\n",
    "MAX_EVALS = 100 # # of hyperopt optimization rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(params):\n",
    "    for param_name in params['int_param_names']:\n",
    "        params['params'][param_name] = int(params['params'][param_name])\n",
    "    \n",
    "    estim = params['model'](**params['params'])\n",
    "    \n",
    "    score = cross_val_score(estim, \n",
    "                            x_train, \n",
    "                            y_train, \n",
    "                            scoring=CV_SCORING_METRIC, \n",
    "                            cv=CV(n_splits=N_SPLITS, \n",
    "                                     shuffle=True, \n",
    "                                     random_state=0)).mean()\n",
    "\n",
    "    return -score if IS_HIGHER_BETTER else score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_estimator():\n",
    "    trials = Trials()\n",
    "    best_estimator = fmin(objective_func,\n",
    "                           space,\n",
    "                           algo=tpe.suggest,\n",
    "                           max_evals=MAX_EVALS, \n",
    "                           trials=trials,\n",
    "                           rstate=np.random.RandomState(0))\n",
    "    \n",
    "    model = space_eval(space, best_estimator)['model']\n",
    "    params = space_eval(space, best_estimator)['params']\n",
    "\n",
    "    estimator = model(**params)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_SCORING_METRIC = 'neg_mean_squared_error'\n",
    "IS_HIGHER_BETTER = True # this will add '-' (minus) to the loss as we have 'NEG' in 'neg_mean_squared_error'\n",
    "                        # because hyperopt tries to MINIMIZE objective function\n",
    "CV = KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = hp.choice('estimator',\n",
    "        [\n",
    "            {'model': LGBMRegressor,\n",
    "             'params': {\n",
    "                        'objective' : hp.choice('objective_LGBM', ['regression']),\n",
    "                        'metric' : hp.choice('metric_LGBM', ['mse']),\n",
    "                        'n_jobs' : hp.choice('n_jobs_LGBM', [3]),\n",
    "                        'random_state' : hp.choice('random_state_LGBM', [0]),\n",
    "\n",
    "                        'colsample_bytree' : hp.uniform('colsample_bytree_LGBM', 0.2, 0.8),\n",
    "                        'learning_rate' : hp.uniform('learning_rate_LGBM', 0.001, 0.5),\n",
    "                        'subsample' : hp.uniform('subsample_LGBM', 0.2, 0.8),\n",
    "\n",
    "                        'max_depth' : hp.quniform('max_depth_LGBM', 2, 9, 1), # 1 stands for q\n",
    "                        'n_estimators' : hp.quniform('n_estimators_LGBM', 50, 500, 1)\n",
    "                      },\n",
    "             'int_param_names' : ['max_depth', 'n_estimators']\n",
    "            },\n",
    "\n",
    "            {'model': Ridge,\n",
    "             'params': {\n",
    "                           'normalize' : hp.choice('normalize_RIDGE', [True, False]),\n",
    "                           'solver' : hp.choice('solver_RIDGE', ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "                           'random_state' : hp.choice('random_state_RIDGE', [0]),\n",
    "\n",
    "                           'alpha' : hp.loguniform('alpha_RIDGE', -5, 5)\n",
    "                      },\n",
    "             'int_param_names' : []\n",
    "        }\n",
    "        ])\n",
    "# Labels should have unique names (i.e. if we put simply 'random_state' in both LGBMRegressor and Ridge -> error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  5.59it/s, best loss: 8.392130215229862]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=0.7182480942124142, importance_type='split',\n",
       "              learning_rate=0.16040147643869565, max_depth=4.0, metric='mse',\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=358.0, n_jobs=3, num_leaves=31,\n",
       "              objective='regression', random_state=0, reg_alpha=0.0,\n",
       "              reg_lambda=0.0, silent=True, subsample=0.7030235795002958,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = get_best_estimator()\n",
    "estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_SCORING_METRIC = 'balanced_accuracy'\n",
    "IS_HIGHER_BETTER = True # this will add '-' (minus) to the loss as we have 'NEG' in 'neg_mean_squared_error'\n",
    "                        # because hyperopt tries to MINIMIZE objective function\n",
    "CV = StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = hp.choice('estimator',\n",
    "        [\n",
    "            {'model': LGBMClassifier,\n",
    "             'params': {\n",
    "                        'objective' : hp.choice('objective_LGBM', ['multiclass']),\n",
    "                        'metric' : hp.choice('metric_LGBM', ['multi_logloss']),\n",
    "                        'n_jobs' : hp.choice('n_jobs_LGBM', [3]),\n",
    "                        'random_state' : hp.choice('random_state_LGBM', [0]),\n",
    "\n",
    "                        'colsample_bytree' : hp.uniform('colsample_bytree_LGBM', 0.2, 0.8),\n",
    "                        'learning_rate' : hp.uniform('learning_rate_LGBM', 0.001, 0.5),\n",
    "                        'subsample' : hp.uniform('subsample_LGBM', 0.2, 0.8),\n",
    "\n",
    "                        'max_depth' : hp.quniform('max_depth_LGBM', 2, 9, 1), # 1 stands for q\n",
    "                        'n_estimators' : hp.quniform('n_estimators_LGBM', 50, 500, 1)\n",
    "                      },\n",
    "             'int_param_names' : ['max_depth', 'n_estimators']\n",
    "            },\n",
    "\n",
    "            {'model': SVC,\n",
    "             'params': {\n",
    "                       'C'      : hp.lognormal('C_SVC', 0,1),\n",
    "                       'kernel' : hp.choice('kernel_SVC', ['rbf', 'poly', 'rbf', 'sigmoid']),\n",
    "                       'degree' : hp.choice('degree_SVC', range(1,15)),\n",
    "                       'gamma'  : hp.uniform('gamma_SVC', 0.001,10000)\n",
    "                      },\n",
    "             'int_param_names' : []\n",
    "        }\n",
    "        ])\n",
    "# Labels should have unique names (i.e. if we put simply 'random_state' in both LGBMRegressor and Ridge -> error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  5.03it/s, best loss: -0.9580687830687831]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=0.6362652862491109, importance_type='split',\n",
       "               learning_rate=0.49849818403162566, max_depth=5.0,\n",
       "               metric='multi_logloss', min_child_samples=20,\n",
       "               min_child_weight=0.001, min_split_gain=0.0, n_estimators=349.0,\n",
       "               n_jobs=3, num_leaves=31, objective='multiclass', random_state=0,\n",
       "               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=0.45608066870051306, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = get_best_estimator()\n",
    "estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
