{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "    \n",
    "https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n",
    "    \n",
    "https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm\n",
    "\n",
    "https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**eta**_ (name comes from greek character η) - learning rate. \n",
    "\n",
    "Makes every successive tree having less impact (thus eta should be less than 1) on the overall prediction which makes the boosting process more conservative (the predictions of the ensemble after next boosting will be more aligned with the predictions after the last boosting step (will not introduce anything \"significantly new\" to the overall ensemble predictions)).\n",
    "\n",
    "Range: [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**gamma**_ - Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "\n",
    "The larger gamma is (meaning we want each split to significantly reduce the loss), the more conservative the algorithm will be (thus reducing the variance). \n",
    "\n",
    "Range: [0, ∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**max_depth**_ - Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit (the variance will be larger!). \n",
    "\n",
    "Range: [0, ∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**min_child_weight**_ - Minimum sum of instance weight (hessian) (or number of samples if all samples have a weight of 1) needed in a child.\n",
    "\n",
    "If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning.\n",
    "\n",
    "In linear regression task (and similar logic applies to other algorithms and tasks), this simply corresponds to minimum number of instances needed to be in each node. Because the loss for each node is 1/2*(y-y_hat)^2, the hessian (for on node it is the second derivative w.r.t. y_hat) is +1. Thus, summing these hessians over the nodes will give us their count.\n",
    "\n",
    "The larger min_child_weight is, the more conservative the algorithm will be (thus reducing the variance).\n",
    "\n",
    "A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit (the variance will be larger!).\n",
    "\n",
    "Range: [0, ∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**subsample**_ - Subsample ratio of the training instances.\n",
    "\n",
    "Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "\n",
    "Range: (0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**colsample_bytree**_ - the subsample ratio of columns when constructing each tree. \n",
    "\n",
    "Subsampling occurs once for every tree constructed.\n",
    "\n",
    "Range: (0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**colsample_bylevel**_ - the subsample ratio of columns for each level. \n",
    "\n",
    "Subsampling occurs once for every new depth level reached in a tree. \n",
    "\n",
    "Columns are subsampled from the set of columns chosen for the current tree.\n",
    "\n",
    "Range: (0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**colsample_bynode**_ - the subsample ratio of columns for each node (split). \n",
    "\n",
    "Subsampling occurs once every time a new split is evaluated. \n",
    "\n",
    "Columns are subsampled from the set of columns chosen for the current level.\n",
    "\n",
    "Range: (0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**lambda**_ - L2 regularization term on weights. \n",
    "\n",
    "Increasing this value will make model more conservative (thus reducing the variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**alpha**_ - L1 regularization term on weights. \n",
    "\n",
    "Increasing this value will make model more conservative (thus reducing the variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**scale_pos_weight**_ - Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "\n",
    "A typical value to consider: sum(negative instances) / sum(positive instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**colsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma vs min_child_weight\n",
    "Always start with 0, use xgb.cv, and look how the train/test are faring. \n",
    "\n",
    "If you train CV skyrocketing over test CV at a blazing speed, this is where Gamma is useful instead of min_child_weight (because you need to control the complexity issued from the loss, not the loss derivative from the hessian weight in min_child_weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma tuning \n",
    "The higher the Gamma, the lower the difference between train/test CV will happen. \n",
    "\n",
    "If you have no idea of the value to use, put 10 and look what happens.\n",
    "\n",
    "If your train/test CV are always lying too close, it means you controlled way too much the complexity of xgboost, and the model can’t grow trees without pruning them (due to the loss threshold not reached thanks to Gamma). Lower Gamma (good relative value to reduce if you don’t know: cut 20% of Gamma away until you test CV grows without having the train CV frozen).\n",
    "\n",
    "If your train/test CV are differing too much, it means you did not control enough the complexity of xgboost, and the model grows too many trees without pruning them (due to the loss threshold not reached because of Gamma). Put a higher Gamma (good absolute value to use if you don’t know: +2, until your test CV can follow faster your train CV which goes slower, your test CV should be able to peak).\n",
    "\n",
    "If your train CV is stuck (not increasing, or increasing way too slowly), decrease Gamma: that value was too high and xgboost keeps pruning trees until it can find something appropriate (or it may end in an endless loop of testing + adding nodes but pruning them straight away…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
